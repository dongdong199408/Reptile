{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import urllib\n",
    "from urllib import request\n",
    "'''\n",
    "Python3.X 动态页面爬取（逆向解析）实例\n",
    "爬取今日头条关键词搜索结果的所有详细页面大图片并按照关键词及文章标题分类存储图片\n",
    "'''\n",
    "\n",
    "class CrawlOptAnalysis(object):\n",
    "    def __init__(self, search_word=\"美女\"):\n",
    "        self.search_word = search_word\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.100 Safari/537.36',\n",
    "            'X-Requested-With': 'XMLHttpRequest',\n",
    "            'Host': 'www.toutiao.com',\n",
    "            'Referer': 'http://www.toutiao.com/search/?keyword={0}'.format(urllib.parse.quote(self.search_word)),\n",
    "            'Accept': 'application/json, text/javascript',\n",
    "        }\n",
    "\n",
    "    def _crawl_data(self, offset):\n",
    "        '''\n",
    "        模拟依据传入 offset 进行分段式上拉加载更多 item 数据爬取\n",
    "        '''\n",
    "        url = 'http://www.toutiao.com/search_content/?offset={0}&format=json&keyword={1}&autoload=true&count=20&cur_tab=1'.format(offset, urllib.parse.quote(self.search_word))\n",
    "       # print(url)\n",
    "        try:\n",
    "            with request.urlopen(url, timeout=10) as response:\n",
    "                content = response.read()\n",
    "        except Exception as e:\n",
    "            content = None\n",
    "            print('crawl data exception.'+str(e))\n",
    "        return content\n",
    "\n",
    "    def _parse_data(self, content):\n",
    "        '''\n",
    "        解析每次上拉加载更多爬取的 item 数据及每个 item 点进去详情页所有大图下载链接\n",
    "        [\n",
    "            {'article_title':XXX, 'article_image_detail':['url1', 'url2', 'url3']},\n",
    "            {'article_title':XXX, 'article_image_detail':['url1', 'url2', 'url3']}\n",
    "        ]\n",
    "        '''\n",
    "        if content is None:\n",
    "            return None\n",
    "        try:\n",
    "            data_list = json.loads(content)['data']\n",
    "            print(data_list)\n",
    "            result_list = list()\n",
    "            for item in data_list:\n",
    "                result_dict = {'article_title': item['title']}\n",
    "                url_list = list()\n",
    "                for url in item['image_detail']:\n",
    "                    url_list.append(url['url'])\n",
    "                result_dict['article_image_detail'] = url_list\n",
    "                result_list.append(result_dict)\n",
    "        except Exception as e:\n",
    "            print('parse data exception.'+str(e))\n",
    "        return result_list\n",
    "\n",
    "    def _save_picture(self, page_title, url):\n",
    "        '''\n",
    "        把爬取的所有大图下载下来\n",
    "        下载目录为./output/search_word/page_title/image_file\n",
    "        '''\n",
    "        if url is None or page_title is None:\n",
    "            print('save picture params is None!')\n",
    "            return\n",
    "        reg_str = r\"[\\/\\\\\\:\\*\\?\\\"\\<\\>\\|]\"  #For Windows File filter: '/\\:*?\"<>|'\n",
    "        page_title = re.sub(reg_str, \"\", page_title)\n",
    "        save_dir = 'd://dataset//output//{0}//{1}//'.format(self.search_word, page_title)\n",
    "        if os.path.exists(save_dir) is False:\n",
    "            os.makedirs(save_dir)\n",
    "        save_file = save_dir + url.split(\"/\")[-1] + '.png'\n",
    "        if os.path.exists(save_file):\n",
    "            return\n",
    "        try:\n",
    "            with request.urlopen(url, timeout=30) as response, open(save_file, 'wb') as f_save:\n",
    "                f_save.write(response.read())\n",
    "            print('Image is saved! search_word={0}, page_title={1}, save_file={2}'.format(self.search_word, page_title, save_file))\n",
    "        except Exception as e:\n",
    "            print('save picture exception.'+str(e))\n",
    "\n",
    "    def go(self):\n",
    "        offset = 0\n",
    "        while True:\n",
    "            page_list = self._parse_data(self._crawl_data(offset))\n",
    "            if page_list is None or len(page_list) <= 0:\n",
    "                break\n",
    "            try:\n",
    "                for page in page_list:\n",
    "                    article_title = page['article_title']\n",
    "                    for img in page['article_image_detail']:\n",
    "                        self._save_picture(article_title, img)\n",
    "            except Exception as e:\n",
    "                print('go exception.'+str(e))\n",
    "            finally:\n",
    "                offset += 20\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #模拟今日头条搜索关键词爬取正文大图\n",
    "    CrawlOptAnalysis(\"美女\").go()\n",
    "    CrawlOptAnalysis(\"旅游\").go()\n",
    "    CrawlOptAnalysis(\"风景\").go()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
